import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import time
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
import os
from torchvision import models, transforms
import json
from sklearn.utils.class_weight import compute_class_weight

# å°†ç±»å®šä¹‰å’Œå‡½æ•°å®šä¹‰æ”¾åœ¨ if __name__ == '__main__' å¤–é¢
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]
        
        if self.transform:
            image = self.transform(image)
        else:
            # é»˜è®¤è½¬æ¢
            image = torch.FloatTensor(image).permute(2, 0, 1)
            
        # å…³é”®ä¿®å¤ï¼šå°†æ ‡ç­¾è½¬æ¢ä¸ºlongç±»å‹
        label = torch.tensor(label, dtype=torch.long)
        
        return image, label

class EfficientGarbageClassifier(nn.Module):
    def __init__(self, num_classes=5):
        super(EfficientGarbageClassifier, self).__init__()
        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        
        # å†»ç»“å‰å‡ å±‚
        for name, param in self.backbone.named_parameters():
            if 'layer1' in name or 'layer2' in name:
                param.requires_grad = False

        # ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(in_features, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.backbone(x)

def validate_model(model, val_loader, criterion, device):
    model.eval()
    correct = 0
    total = 0
    val_loss = 0

    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            loss = criterion(outputs, target)
            val_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    return val_loss / len(val_loader), correct / total

if __name__ == '__main__':
    print("=== åƒåœ¾åˆ†ç±»æ¨¡å‹è®­ç»ƒ ===")
    
    # ç¡®ä¿modelsç›®å½•å­˜åœ¨
    os.makedirs('models', exist_ok=True)

    # 1. åŠ è½½æ•°æ®
    print("1. åŠ è½½é¢„å¤„ç†çš„æ•°æ®...")
    X_train = np.load('data/processed/splits/X_train.npy')
    y_train = np.load('data/processed/splits/y_train.npy')
    X_val = np.load('data/processed/splits/X_val.npy')
    y_val = np.load('data/processed/splits/y_val.npy')

    print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}")
    print(f"éªŒè¯é›†å½¢çŠ¶: {X_val.shape}")

    # 2. æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†
    print("2. å‡†å¤‡æ•°æ®å¢å¼ºå’Œæ•°æ®åŠ è½½å™¨...")

    # å®šä¹‰æ•°æ®å¢å¼ºå˜æ¢
    train_transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = CustomDataset(X_train, y_train, transform=train_transform)
    val_dataset = CustomDataset(X_val, y_val, transform=val_transform)

    # è®¡ç®—ç±»åˆ«æƒé‡
    print("è®¡ç®—ç±»åˆ«æƒé‡...")
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weights = torch.FloatTensor(class_weights)
    print(f"ç±»åˆ«æƒé‡: {class_weights}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - åœ¨Windowsä¸Šå»ºè®®ä½¿ç”¨num_workers=0
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)

    # 3. åˆ›å»ºæ¨¡å‹
    print("3. åˆ›å»ºæ¨¡å‹...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    model = EfficientGarbageClassifier(num_classes=5)
    model = model.to(device)

    # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å†»ç»“å‚æ•°æ¯”ä¾‹: {(total_params - trainable_params) / total_params:.2%}")

    # 4. è®­ç»ƒæ¨¡å‹
    print("4. å¼€å§‹è®­ç»ƒ...")
    # ä½¿ç”¨å¸¦æƒé‡çš„æŸå¤±å‡½æ•°å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # ä½¿ç”¨AdamW

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½¿ç”¨ä½™å¼¦é€€ç«
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)

    # è®°å½•è®­ç»ƒè¿‡ç¨‹
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    learning_rates = []

    best_val_acc = 0.0
    patience = 15  # å¢åŠ è€å¿ƒå€¼
    patience_counter = 0

    start_time = time.time()

    print("å¼€å§‹è®­ç»ƒå¾ªç¯...")
    for epoch in range(100):  # å¢åŠ æœ€å¤§epochæ•°
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        learning_rates.append(current_lr)

        train_loss = running_loss / len(train_loader)
        train_acc = correct / total

        # éªŒè¯é˜¶æ®µ
        val_loss, val_acc = validate_model(model, val_loader, criterion, device)

        # è®°å½•æŒ‡æ ‡
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_acc': val_acc,
                'train_loss': train_loss,
                'val_loss': val_loss,
                'class_weights': class_weights,
                'train_acc': train_acc
            }, 'models/best_model.pth')
            print(f"âœ… Epoch {epoch + 1}: ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
        else:
            patience_counter += 1

        # æ—©åœæ£€æŸ¥
        if patience_counter >= patience:
            print(f"ğŸ›‘ æ—©åœè§¦å‘! åœ¨ epoch {epoch + 1}")
            break

        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 2 == 0:  # æ›´é¢‘ç¹çš„æ‰“å°
            print(f'Epoch {epoch + 1}/100:')
            print(f'  è®­ç»ƒ: æŸå¤±={train_loss:.4f}, å‡†ç¡®ç‡={train_acc:.4f}')
            print(f'  éªŒè¯: æŸå¤±={val_loss:.4f}, å‡†ç¡®ç‡={val_acc:.4f}')
            print(f'  å­¦ä¹ ç‡: {current_lr:.6f}, æ—©åœ: {patience_counter}/{patience}')

    training_time = time.time() - start_time
    print(f"è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {training_time:.2f}ç§’")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")

    # 5. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    print("5. ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    plt.figure(figsize=(18, 6))

    plt.subplot(1, 3, 1)
    plt.plot(train_losses, label='Train Loss', color='blue', alpha=0.7, linewidth=2)
    plt.plot(val_losses, label='Val Loss', color='red', alpha=0.7, linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training and Validation Loss')
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 3, 2)
    plt.plot(train_accuracies, label='Train Accuracy', color='blue', alpha=0.7, linewidth=2)
    plt.plot(val_accuracies, label='Validation Accuracy', color='red', alpha=0.7, linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Training and Validation Accuracy')
    plt.grid(True, alpha=0.3)

    # æ ‡è®°æœ€ä½³å‡†ç¡®ç‡
    best_epoch = np.argmax(val_accuracies)
    plt.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7)
    plt.text(best_epoch, 0.5, f'best\n{val_accuracies[best_epoch]:.3f}',
             rotation=0, transform=plt.gca().get_xaxis_transform(),
             ha='center', va='center')

    plt.subplot(1, 3, 3)
    plt.plot(learning_rates, color='purple', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate Schedule')
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

    plt.tight_layout()
    plt.savefig('models/training_history.png', dpi=300, bbox_inches='tight')
    print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° models/training_history.png")
    plt.show()

    # 6. åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    print("6. åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    checkpoint = torch.load('models/best_model.pth')
    model.load_state_dict(checkpoint['model_state_dict'])

    # åœ¨éªŒè¯é›†ä¸Šæœ€ç»ˆè¯„ä¼°
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(target.cpu().numpy())

    # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
    final_accuracy = np.mean(np.array(all_preds) == np.array(all_targets))
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_accuracy:.4f}")

    # 7. ä¿å­˜è®­ç»ƒæ€»ç»“
    training_summary = {
        'best_validation_accuracy': float(best_val_acc),
        'final_validation_accuracy': float(final_accuracy),
        'total_training_time': float(training_time),
        'total_epochs_trained': len(train_losses),
        'best_epoch': int(best_epoch),
        'model_architecture': 'ResNet18',
        'num_classes': 5,  # æ˜ç¡®è®°å½•ç±»åˆ«æ•°
        'image_size': [224, 224],
        'device_used': str(device),
        'trainable_parameters': trainable_params,
        'total_parameters': total_params,
        'class_weights': class_weights.tolist(),
        'training_parameters': {
            'batch_size': 16,
            'initial_learning_rate': 0.001,
            'weight_decay': 1e-4,
            'optimizer': 'AdamW',
            'scheduler': 'CosineAnnealingLR (T_max=50)',
            'loss_function': 'CrossEntropyLoss with class weights',
            'early_stopping_patience': patience,
            'data_augmentation': True,
            'gradient_clipping': True
        }
    }

    with open('models/training_summary.json', 'w', encoding='utf-8') as f:
        json.dump(training_summary, f, indent=2, ensure_ascii=False)

    print("è®­ç»ƒæ€»ç»“å·²ä¿å­˜åˆ° models/training_summary.json")
    print("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆ!")