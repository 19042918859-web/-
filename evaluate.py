import torch
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns
import json
import warnings
import torch.nn as nn
from torchvision import models, transforms
import pandas as pd
from PIL import Image

print("=== æ­¥éª¤3: æ¨¡å‹è¯„ä¼° ===")

# è®¾ç½®matplotlib
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å¿½ç•¥å­—ä½“è­¦å‘Šå’Œtorchvisionå¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# 1. åŠ è½½æµ‹è¯•æ•°æ®
print("1. åŠ è½½æµ‹è¯•æ•°æ®...")
X_test = np.load('data/processed/splits/X_test.npy')
y_test = np.load('data/processed/splits/y_test.npy')

# å®šä¹‰ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é¢„å¤„ç†
test_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]
        
        if self.transform:
            image = self.transform(image)
        else:
            image = torch.FloatTensor(image).permute(2, 0, 1)
        
        # å…³é”®ä¿®å¤ï¼šå°†æ ‡ç­¾è½¬æ¢ä¸ºlongç±»å‹
        label = torch.tensor(label, dtype=torch.long)
            
        return image, label

# 2. åŠ è½½æ¨¡å‹
print("2. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")

class EfficientGarbageClassifier(nn.Module):
    def __init__(self, num_classes=5):  # ä¿®æ­£ï¼šæ”¹ä¸º5ç±»
        super(EfficientGarbageClassifier, self).__init__()

        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)

        # å†»ç»“å‰å‡ å±‚ - ä¸è®­ç»ƒæ—¶ä¸€è‡´
        for name, param in self.backbone.named_parameters():
            if 'layer1' in name or 'layer2' in name:
                param.requires_grad = False

        # ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚ - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(in_features, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.backbone(x)

def main():
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = CustomDataset(X_test, y_test, transform=test_transform)
    # ç¦ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = EfficientGarbageClassifier(num_classes=5)  # ä¿®æ­£ï¼šæ”¹ä¸º5ç±»
    model = model.to(device)

    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    print("åŠ è½½æ¨¡å‹æƒé‡...")
    checkpoint = torch.load('models/best_model.pth', map_location=device, weights_only=False)

    # æ›´çµæ´»çš„æƒé‡åŠ è½½
    model_state_dict = model.state_dict()
    pretrained_dict = checkpoint['model_state_dict']

    # åªåŠ è½½åŒ¹é…çš„é”®
    matched_keys = []
    unmatched_keys = []
    for name, param in pretrained_dict.items():
        if name in model_state_dict:
            if model_state_dict[name].shape == param.shape:
                model_state_dict[name] = param
                matched_keys.append(name)
            else:
                unmatched_keys.append(f"å½¢çŠ¶ä¸åŒ¹é…: {name} - æ¨¡å‹: {model_state_dict[name].shape}, æ£€æŸ¥ç‚¹: {param.shape}")
        else:
            unmatched_keys.append(f"ç¼ºå¤±é”®: {name}")

    print(f"æˆåŠŸåŒ¹é… {len(matched_keys)}/{len(model_state_dict)} ä¸ªå‚æ•°")
    if unmatched_keys:
        print("ä¸åŒ¹é…çš„å‚æ•°:")
        for key in unmatched_keys[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  {key}")

    model.load_state_dict(model_state_dict, strict=False)
    model.eval()
    print("âœ… æ¨¡å‹å‡†å¤‡å®Œæˆ")

    # 3. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print("3. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
    all_predictions = []
    all_targets = []
    all_probabilities = []
    test_loss = 0.0
    criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            loss = criterion(outputs, target)
            test_loss += loss.item()

            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)

            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())

    test_loss /= len(test_loader)

    # 4. è®¡ç®—æŒ‡æ ‡
    print("4. è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    accuracy = accuracy_score(all_targets, all_predictions)

    # åŠ è½½ç±»åˆ«ä¿¡æ¯
    with open('data/processed/dataset_info.json', 'r') as f:
        dataset_info = json.load(f)
    classes = dataset_info['classes']

    # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
    precision, recall, f1, support = precision_recall_fscore_support(
        all_targets, all_predictions, average=None, labels=range(len(classes))
    )

    # è®¡ç®—å®å¹³å‡å’ŒåŠ æƒå¹³å‡
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
        all_targets, all_predictions, average='macro'
    )
    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
        all_targets, all_predictions, average='weighted'
    )

    print(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"å®å¹³å‡ F1: {f1_macro:.4f}")
    print(f"åŠ æƒå¹³å‡ F1: {f1_weighted:.4f}")

    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_targets, all_predictions, target_names=classes))

    # 5. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    print("5. ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
    cm = confusion_matrix(all_targets, all_predictions)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes,
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
    plt.title('Confusion Matrix - åƒåœ¾åˆ†ç±»æ¨¡å‹', fontsize=16, pad=20)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.tight_layout()
    plt.savefig('models/confusion_matrix.png', dpi=300, bbox_inches='tight')
    print("æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ° models/confusion_matrix.png")
    plt.show()

    # 6. ç»˜åˆ¶ç±»åˆ«æ€§èƒ½å¯¹æ¯”å›¾
    print("6. ç»˜åˆ¶ç±»åˆ«æ€§èƒ½å¯¹æ¯”å›¾...")
    metrics_df = pd.DataFrame({
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1
    }, index=classes)

    plt.figure(figsize=(12, 6))
    x = np.arange(len(classes))
    width = 0.25

    plt.bar(x - width, metrics_df['Precision'], width, label='Precision', alpha=0.8)
    plt.bar(x, metrics_df['Recall'], width, label='Recall', alpha=0.8)
    plt.bar(x + width, metrics_df['F1-Score'], width, label='F1-Score', alpha=0.8)

    plt.xlabel('åƒåœ¾ç±»åˆ«')
    plt.ylabel('åˆ†æ•°')
    plt.title('å„ç±»åˆ«æ€§èƒ½å¯¹æ¯”')
    plt.xticks(x, classes)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('models/class_performance.png', dpi=300, bbox_inches='tight')
    print("ç±»åˆ«æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜åˆ° models/class_performance.png")
    plt.show()

    # 7. è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    print("7. è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡...")
    class_correct = [0] * len(classes)
    class_total = [0] * len(classes)
    class_confidences = [[] for _ in range(len(classes))]

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            confidences, predicted = torch.max(probabilities, 1)

            for i in range(len(target)):
                label = target[i]
                class_correct[label] += (predicted[i] == label).item()
                class_total[label] += 1
                class_confidences[label].append(confidences[i].item())

    class_accuracies = {}
    class_avg_confidences = {}
    for i in range(len(classes)):
        if class_total[i] > 0:
            accuracy = 100 * class_correct[i] / class_total[i]
            avg_confidence = np.mean(class_confidences[i]) * 100 if class_confidences[i] else 0
            class_accuracies[classes[i]] = round(accuracy, 2)
            class_avg_confidences[classes[i]] = round(avg_confidence, 2)
            print(f'{classes[i]}: {accuracy:.2f}% ({class_correct[i]}/{class_total[i]}), å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f}%')

    # 8. ç”ŸæˆKPIæŠ¥å‘Š
    print("8. ç”ŸæˆKPIæŠ¥å‘Š...")
    
    # ä¿®å¤ï¼šå®‰å…¨åœ°è·å–æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ
    try:
        # å°è¯•ä»ä¸åŒå¯èƒ½çš„ä½ç½®è·å–æµ‹è¯•é›†åˆ†å¸ƒ
        if 'class_distribution' in dataset_info and 'test' in dataset_info['class_distribution']:
            class_distribution = dataset_info['class_distribution']['test']
        elif 'class_distribution' in dataset_info and 'augmented' in dataset_info['class_distribution']:
            class_distribution = dataset_info['class_distribution']['augmented']['test']
        elif 'sizes' in dataset_info and 'test' in dataset_info['sizes']:
            # å¦‚æœæ²¡æœ‰ç±»åˆ«åˆ†å¸ƒï¼Œè‡³å°‘è®°å½•æµ‹è¯•é›†å¤§å°
            class_distribution = {"total": dataset_info['sizes']['test']}
        else:
            # å¦‚æœéƒ½æ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨æˆ‘ä»¬è®¡ç®—çš„å®é™…åˆ†å¸ƒ
            class_distribution = {classes[i]: class_total[i] for i in range(len(classes))}
    except KeyError:
        # å¦‚æœå‡ºç°ä»»ä½•é”™è¯¯ï¼Œä½¿ç”¨æˆ‘ä»¬è®¡ç®—çš„å®é™…åˆ†å¸ƒ
        class_distribution = {classes[i]: class_total[i] for i in range(len(classes))}
    
    kpi_report = {
        'test_accuracy': float(accuracy),
        'test_loss': float(test_loss),
        'test_size': len(y_test),
        'macro_f1': float(f1_macro),
        'weighted_f1': float(f1_weighted),
        'macro_precision': float(precision_macro),
        'macro_recall': float(recall_macro),
        'class_distribution': class_distribution,
        'class_accuracies': class_accuracies,
        'class_confidences': class_avg_confidences,
        'class_metrics': {
            'precision': {classes[i]: float(precision[i]) for i in range(len(classes))},
            'recall': {classes[i]: float(recall[i]) for i in range(len(classes))},
            'f1_score': {classes[i]: float(f1[i]) for i in range(len(classes))}
        },
        'best_validation_accuracy': float(checkpoint.get('val_acc', 0)),
        'model_architecture': 'ResNet18',
        'training_epoch': checkpoint.get('epoch', 0),
        'evaluation_timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    }

    print("\n=== KPI æŠ¥å‘Š ===")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {kpi_report['test_accuracy']:.4f}")
    print(f"æµ‹è¯•æŸå¤±: {kpi_report['test_loss']:.4f}")
    print(f"å®å¹³å‡ F1: {kpi_report['macro_f1']:.4f}")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {kpi_report['best_validation_accuracy']:.4f}")
    print("\nå„ç±»åˆ«å‡†ç¡®ç‡:")
    for class_name, acc in kpi_report['class_accuracies'].items():
        conf = kpi_report['class_confidences'].get(class_name, 0)
        print(f"  {class_name}: {acc}% (å¹³å‡ç½®ä¿¡åº¦: {conf}%)")

    with open('models/kpi_report.json', 'w', encoding='utf-8') as f:
        json.dump(kpi_report, f, indent=2, ensure_ascii=False)

    print("\nKPIæŠ¥å‘Šå·²ä¿å­˜åˆ° models/kpi_report.json")
    print("è¯„ä¼°å®Œæˆ! ğŸ‰")

    # 9. ç”Ÿæˆæ€§èƒ½æ€»ç»“
    print("\n=== æ€§èƒ½æ€»ç»“ ===")
    print(f"æ€»ä½“å‡†ç¡®ç‡: {accuracy:.2%}")
    print(f"æ¨¡å‹é²æ£’æ€§: {'ä¼˜ç§€' if f1_macro > 0.85 else 'è‰¯å¥½' if f1_macro > 0.75 else 'ä¸€èˆ¬'}")
    print(f"ç±»åˆ«å¹³è¡¡æ€§: {'è‰¯å¥½' if min(precision) > 0.7 and min(recall) > 0.7 else 'éœ€è¦æ”¹è¿›'}")

    # æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„ç±»åˆ«
    best_class = classes[np.argmax(f1)]
    worst_class = classes[np.argmin(f1)]
    print(f"è¡¨ç°æœ€ä½³ç±»åˆ«: {best_class} (F1: {np.max(f1):.3f})")
    print(f"è¡¨ç°æœ€å·®ç±»åˆ«: {worst_class} (F1: {np.min(f1):.3f})")

if __name__ == '__main__':
    main()